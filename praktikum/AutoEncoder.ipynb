{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from myutils import Preprocessing as pre\n",
    "from myutils import Datasets as ds\n",
    "from myutils import Helpers as h\n",
    "from myutils import Complexity_Measures as cm\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Magic Command, so changes in myutils module are reloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport myutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = ds.ALL_NUMBERS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing subset: 0,1\n",
      "396/396 [==============================] - 4s 9ms/step - loss: 0.0524\n",
      "processing subset: 0,2\n",
      "372/372 [==============================] - 3s 8ms/step - loss: 0.0853\n",
      "processing subset: 0,3\n",
      "377/377 [==============================] - 4s 9ms/step - loss: 0.0918\n",
      "processing subset: 0,4\n",
      "368/368 [==============================] - 3s 9ms/step - loss: 0.0918\n",
      "processing subset: 0,5\n",
      "355/355 [==============================] - 3s 9ms/step - loss: 0.0795\n",
      "processing subset: 0,6\n",
      "371/371 [==============================] - 3s 8ms/step - loss: 0.0980\n",
      "processing subset: 0,7\n",
      "381/381 [==============================] - 3s 8ms/step - loss: 0.0819\n",
      "processing subset: 0,8\n",
      "368/368 [==============================] - 3s 8ms/step - loss: 0.0897\n",
      "processing subset: 0,9\n",
      "371/371 [==============================] - 3s 6ms/step - loss: 0.0712\n",
      "processing subset: 1,2\n",
      "397/397 [==============================] - 4s 8ms/step - loss: 0.0494\n",
      "processing subset: 1,3\n",
      "403/403 [==============================] - 4s 8ms/step - loss: 0.0492\n",
      "processing subset: 1,4\n",
      "394/394 [==============================] - 3s 8ms/step - loss: 0.0523\n",
      "processing subset: 1,5\n",
      "381/381 [==============================] - 3s 8ms/step - loss: 0.0673\n",
      "processing subset: 1,6\n",
      "396/396 [==============================] - 4s 8ms/step - loss: 0.0529\n",
      "processing subset: 1,7\n",
      "407/407 [==============================] - 4s 8ms/step - loss: 0.0422\n",
      "processing subset: 1,8\n",
      "394/394 [==============================] - 3s 8ms/step - loss: 0.0683\n",
      "processing subset: 1,9\n",
      "397/397 [==============================] - 4s 8ms/step - loss: 0.0469\n",
      "processing subset: 2,3\n",
      "378/378 [==============================] - 3s 8ms/step - loss: 0.0835\n",
      "processing subset: 2,4\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.0865\n",
      "processing subset: 2,5\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 0.0789\n",
      "processing subset: 2,6\n",
      "372/372 [==============================] - 4s 9ms/step - loss: 0.0692\n",
      "processing subset: 2,7\n",
      "382/382 [==============================] - 4s 9ms/step - loss: 0.0652\n",
      "processing subset: 2,8\n",
      "370/370 [==============================] - 3s 8ms/step - loss: 0.0928\n",
      "processing subset: 2,9\n",
      "373/373 [==============================] - 4s 9ms/step - loss: 0.0845\n",
      "processing subset: 3,4\n",
      "375/375 [==============================] - 4s 8ms/step - loss: 0.0808\n",
      "processing subset: 3,5\n",
      "361/361 [==============================] - 3s 7ms/step - loss: 0.0699\n",
      "processing subset: 3,6\n",
      "377/377 [==============================] - 4s 9ms/step - loss: 0.0746\n",
      "processing subset: 3,7\n",
      "388/388 [==============================] - 4s 9ms/step - loss: 0.0651\n",
      "processing subset: 3,8\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0869\n",
      "processing subset: 3,9\n",
      "378/378 [==============================] - 4s 9ms/step - loss: 0.0850\n",
      "processing subset: 4,5\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 0.0702\n",
      "processing subset: 4,6\n",
      "368/368 [==============================] - 3s 9ms/step - loss: 0.0717\n",
      "processing subset: 4,7\n",
      "379/379 [==============================] - 3s 8ms/step - loss: 0.0593\n",
      "processing subset: 4,8\n",
      "366/366 [==============================] - 3s 8ms/step - loss: 0.0718\n",
      "processing subset: 4,9\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.0670\n",
      "processing subset: 5,6\n",
      "355/355 [==============================] - 4s 8ms/step - loss: 0.0653\n",
      "processing subset: 5,7\n",
      "366/366 [==============================] - 3s 9ms/step - loss: 0.0583\n",
      "processing subset: 5,8\n",
      "353/353 [==============================] - 3s 9ms/step - loss: 0.0754\n",
      "processing subset: 5,9\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 0.0761\n",
      "processing subset: 6,7\n",
      "381/381 [==============================] - 4s 9ms/step - loss: 0.0705\n",
      "processing subset: 6,8\n",
      "368/368 [==============================] - 3s 8ms/step - loss: 0.0793\n",
      "processing subset: 6,9\n",
      "371/371 [==============================] - 3s 8ms/step - loss: 0.0816\n",
      "processing subset: 7,8\n",
      "379/379 [==============================] - 4s 8ms/step - loss: 0.0816\n",
      "processing subset: 7,9\n",
      "382/382 [==============================] - 4s 8ms/step - loss: 0.0678\n",
      "processing subset: 8,9\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.0739\n"
     ]
    }
   ],
   "source": [
    "preprocessing_name = \"Autoencoder\"\n",
    "\n",
    "\n",
    "os.system(\"rm -rf data/\"+preprocessing_name+\"/*\")\n",
    "\n",
    "for subset in itertools.combinations([i for i in range(10)],2):\n",
    "\n",
    "\n",
    "    #mkdir\n",
    "    subset_name = str(subset).replace(\"(\",\"\").replace(\")\",\"\").replace(\" \",\"\")\n",
    "    os.mkdir(\"data/{}/{}\".format(preprocessing_name,subset_name))\n",
    "    \n",
    "    #log progress\n",
    "    print(\"processing subset:\",subset_name)\n",
    "\n",
    "    x_train_subset = x_train[(y_train == subset[0]) | (y_train == subset[1])]\n",
    "    y_train_subset = y_train[(y_train == subset[0]) | (y_train == subset[1])]\n",
    "    y_train_subset_binary = np.where(y_train_subset == subset[0], 0, y_train_subset)\n",
    "    y_train_subset_binary = np.where(y_train_subset_binary == subset[1], 1, y_train_subset_binary)\n",
    "\n",
    "    x_test_subset = x_test[(y_test == subset[0]) | (y_test == subset[1])]\n",
    "    y_test_subset = y_test[(y_test == subset[0]) | (y_test == subset[1])]\n",
    "    y_test_subset_binary = np.where(y_test_subset == subset[0], 0, y_test_subset)\n",
    "    y_test_subset_binary = np.where(y_test_subset_binary == subset[1], 1, y_test_subset_binary)\n",
    "\n",
    "    x_train_subset = x_train_subset.reshape(x_train_subset.shape[0],784)\n",
    "    x_test_subset = x_test_subset.reshape(x_test_subset.shape[0],784)\n",
    "\n",
    "    x_train_pre , x_test_pre, hist = pre.Autoencoder(x_train_subset,x_test_subset,outputsize=4,epochs=1)\n",
    "    loss = np.array(hist.history[\"loss\"])\n",
    "    \n",
    "    #scale between 0 and 1\n",
    "    #x_train_pre = pre.minmax_scaler(x_train_pre,min=0,max=1)\n",
    "    #x_test_pre = pre.minmax_scaler(x_test_pre,min=0,max=1)\n",
    "\n",
    "    for type, dataset in zip([\"x_train\",\"x_test\",\"y_train\",\"y_test\",\"y_train_binary\",\"y_test_binary\",\"loss\"],[x_train_pre,x_test_pre,y_train_subset,y_test_subset,y_train_subset_binary,y_test_subset_binary,loss]):\n",
    "        np.save(\"data/\"+preprocessing_name+\"/\"+str(subset_name)+\"/\"+type+\".npy\",dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw not found\n"
     ]
    }
   ],
   "source": [
    "data = ds.get_preprocessed_datasets(\"Autoencoder\",\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dict() \n",
    "for subset in sorted(data[\"Autoencoder\"]):\n",
    "\n",
    "    fischer = cm.fischer_discriminat_ratio(x=data[\"Autoencoder\"][subset][\"x_train\"].astype(\"int\"),y=data[\"Autoencoder\"][subset][\"y_train\"])\n",
    "    result[subset] = fischer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0,1</th>\n",
       "      <td>0.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0,2</th>\n",
       "      <td>0.359088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0,3</th>\n",
       "      <td>0.316373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0,4</th>\n",
       "      <td>0.455648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0,5</th>\n",
       "      <td>0.515672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0,6</th>\n",
       "      <td>0.623304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0,7</th>\n",
       "      <td>0.183641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0,8</th>\n",
       "      <td>0.164072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0,9</th>\n",
       "      <td>0.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1,2</th>\n",
       "      <td>0.401258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1,3</th>\n",
       "      <td>0.304414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1,4</th>\n",
       "      <td>0.343676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1,5</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1,6</th>\n",
       "      <td>0.425066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1,7</th>\n",
       "      <td>0.642573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1,8</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1,9</th>\n",
       "      <td>0.231976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2,3</th>\n",
       "      <td>0.498330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2,4</th>\n",
       "      <td>0.872318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2,5</th>\n",
       "      <td>0.332142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2,6</th>\n",
       "      <td>0.277980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2,7</th>\n",
       "      <td>0.242331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2,8</th>\n",
       "      <td>0.650394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2,9</th>\n",
       "      <td>0.316031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3,4</th>\n",
       "      <td>0.286497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3,5</th>\n",
       "      <td>0.743608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3,6</th>\n",
       "      <td>0.218709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3,7</th>\n",
       "      <td>0.275375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3,8</th>\n",
       "      <td>0.984609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3,9</th>\n",
       "      <td>0.929659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4,5</th>\n",
       "      <td>0.215695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4,6</th>\n",
       "      <td>0.501221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4,7</th>\n",
       "      <td>0.321092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4,8</th>\n",
       "      <td>0.264459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4,9</th>\n",
       "      <td>0.911427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5,6</th>\n",
       "      <td>0.277360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5,7</th>\n",
       "      <td>0.304301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5,8</th>\n",
       "      <td>0.517638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5,9</th>\n",
       "      <td>0.339432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6,7</th>\n",
       "      <td>0.261944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6,8</th>\n",
       "      <td>0.396494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6,9</th>\n",
       "      <td>0.984121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7,8</th>\n",
       "      <td>0.404410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7,9</th>\n",
       "      <td>0.885732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8,9</th>\n",
       "      <td>0.420548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0,1  0.178600\n",
       "0,2  0.359088\n",
       "0,3  0.316373\n",
       "0,4  0.455648\n",
       "0,5  0.515672\n",
       "0,6  0.623304\n",
       "0,7  0.183641\n",
       "0,8  0.164072\n",
       "0,9  0.541000\n",
       "1,2  0.401258\n",
       "1,3  0.304414\n",
       "1,4  0.343676\n",
       "1,5  1.000000\n",
       "1,6  0.425066\n",
       "1,7  0.642573\n",
       "1,8  1.000000\n",
       "1,9  0.231976\n",
       "2,3  0.498330\n",
       "2,4  0.872318\n",
       "2,5  0.332142\n",
       "2,6  0.277980\n",
       "2,7  0.242331\n",
       "2,8  0.650394\n",
       "2,9  0.316031\n",
       "3,4  0.286497\n",
       "3,5  0.743608\n",
       "3,6  0.218709\n",
       "3,7  0.275375\n",
       "3,8  0.984609\n",
       "3,9  0.929659\n",
       "4,5  0.215695\n",
       "4,6  0.501221\n",
       "4,7  0.321092\n",
       "4,8  0.264459\n",
       "4,9  0.911427\n",
       "5,6  0.277360\n",
       "5,7  0.304301\n",
       "5,8  0.517638\n",
       "5,9  0.339432\n",
       "6,7  0.261944\n",
       "6,8  0.396494\n",
       "6,9  0.984121\n",
       "7,8  0.404410\n",
       "7,9  0.885732\n",
       "8,9  0.420548"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(result,orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape_shape (12593, 2, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot grid of 1,8\n",
    "h.plot_grid(data[\"Autoencoder\"][\"1,8\"][\"x_train\"],data[\"Autoencoder\"][\"1,8\"][\"y_train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the pair of numbers 1 and 8 manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "394/394 [==============================] - 3s 7ms/step - loss: 0.0579\n"
     ]
    }
   ],
   "source": [
    "x_train_18 = x_train[(y_train == 1) | (y_train == 8)]\n",
    "y_train_18 = y_train[(y_train == 1) | (y_train == 8)]\n",
    "y_test_18 = y_test[(y_test == 1) | (y_test == 8)]\n",
    "x_test_18 = x_test[(y_test == 1) | (y_test == 8)]\n",
    "\n",
    "x_train_18 = x_train_18.reshape(x_train_18.shape[0],784)\n",
    "x_test_18 = x_test_18.reshape(x_test_18.shape[0],784)\n",
    "\n",
    "x_train_18_pre , x_test_18_pre, hist = pre.Autoencoder(x_train_18,x_test_18,outputsize=4,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20281031065117897"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fischer_discriminat_ratio(x=x_train_18_pre,y=y_train_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape_shape (12593, 2, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h.plot_grid(x_train_18_pre,y_train_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66fbc7444ff33bf974f21d3d224fe531a78305e3f7b70f0cb67a92233fdb1f5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tfm1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
